{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the datset, replacing the file path with your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "MyDeal = pd.read_excel(r\"C:\\Users\\justi\\OneDrive\\Documents\\UT_Capstone_Data_Set (2).xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify won and lost deals as 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "Train = MyDeal.loc[(MyDeal.DealStatusValue == 'Won') | (MyDeal.DealStatusValue == 'Lost')]\n",
    "\n",
    "# Test = MyDeal.loc[(MyDeal.DealStatusValue == 'Deliverable Provided')| (MyDeal.DealStatusValue == 'Solutioning')]\n",
    "\n",
    "\n",
    "Train['Won'] = np.where(Train['DealStatusValue']== 'Lost', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Governance Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turns governance into a binary - either governance (1) or no governance (0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train['Governance'] = np.where(Train['GovernanceStatusName']== 'No Governance', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapsing deals with multiple rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we manage deals with multiple rows. This chunk combines multiple row deals, and creates a 'DealTechnologies' column simply indicating the number of technologies in a given deal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate technology complexity\n",
    "complexity = Train.groupby('MyDealId',as_index=True).size()\n",
    "complexity_dict = complexity.to_dict()\n",
    "\n",
    " \n",
    "\n",
    "## Checking for duplicate id's\n",
    "mult_id_list = Train[Train.duplicated(['MyDealId'])]\n",
    "mult_id_list = mult_id_list[mult_id_list['IsPrimary'] == 1]\n",
    "mult_id_list = mult_id_list['MyDealId'].tolist()\n",
    "Train['ComplexityFlag'] = np.where(Train['MyDealId'].isin(mult_id_list), 1, 0)\n",
    "\n",
    " \n",
    "\n",
    "##Keep only primary entries\n",
    "Train = Train[Train['IsPrimary'] == 1]\n",
    "\n",
    " \n",
    "\n",
    "# add technolidy complexity column\n",
    "Train['DealTechnologies'] = Train['MyDealId'].map(complexity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to solution is calculated by taking the difference betweeen FinalDeliverableProvidedDate (or FirstDeliverableProvidedDate if this field is null) and IntakeDate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train['DeliverableProvidedNew'] =  Train['FirstDeliverableProvidedDate'].mask(pd.isnull, Train['FinalDeliverableProvidedDate'])\n",
    "\n",
    "\n",
    "Train['DeliverableProvidedNew'] = pd.to_datetime(Train['DeliverableProvidedNew'], errors='coerce')\n",
    "Train['IntakeDates1'] = pd.to_datetime(Train['IntakeDate'], errors='coerce')\n",
    "\n",
    "\n",
    "Train['TimeToSolution'] = (Train['DeliverableProvidedNew']-Train['IntakeDates1']).dt.days\n",
    "\n",
    "def negative_replace(x):\n",
    "    if x<0:\n",
    "        return 0 \n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "Train['TimeToSolution'] = Train['TimeToSolution'].map(negative_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating the year out from the intake date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        2020\n",
       "3        2020\n",
       "6        2020\n",
       "7        2020\n",
       "9        2020\n",
       "         ... \n",
       "43328    2020\n",
       "43329    2020\n",
       "43333    2020\n",
       "43334    2020\n",
       "43336    2020\n",
       "Name: IntakeDates_Year, Length: 24577, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Year Column\n",
    "import numpy as np\n",
    "\n",
    "Train['IntakeDates_Year'] = Train['IntakeDate'].astype(str).str[0:4]\n",
    "\n",
    "\n",
    "\n",
    "Train['IntakeDates_Year']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferring fiscal quarter from intake date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        1\n",
       "3        1\n",
       "6        3\n",
       "7        1\n",
       "9        2\n",
       "        ..\n",
       "43328    2\n",
       "43329    1\n",
       "43333    1\n",
       "43334    4\n",
       "43336    4\n",
       "Name: Fiscal_Quarter, Length: 24577, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating Quarter Variable\n",
    "\n",
    "def month(x):\n",
    "    x = x.month\n",
    "    return x\n",
    "\n",
    "Train['Fiscal_Quarter'] = Train.IntakeDate.map(month)\n",
    "\n",
    "Train['Fiscal_Quarter'] = Train.Fiscal_Quarter.replace([2,3,4,5,6,7,8,9,10,11,12,1],[1,1,1,2,2,2,3,3,3,4,4,4])\n",
    "\n",
    "Train['Fiscal_Quarter'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to run this cell - it indicates that we are using all of the data as the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data = Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation for price vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below saves mean and median info for later transformations of test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cc = Train['ContractCost'].mean()\n",
    "mean_cm = Train['ContractMargin'].mean()\n",
    "med_fr = Train['ForecastedRevenue'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are imputuing null values for Contract Cost (mean), Contract Margin (Mean) and Forecasted Revenue (Median). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "Train[['ContractMargin']]=imputer.fit_transform(Train[['ContractMargin']])\n",
    "Train[['ContractCost']]=imputer.fit_transform(Train[['ContractCost']])\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "Train[['ForecastedRevenue']]=imputer.fit_transform(Train[['ForecastedRevenue']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract Cost Bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bins contract costs into 4 equally sized based on quartile. Bin 1 is low, Bin 4 is high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        4\n",
       "3        1\n",
       "6        3\n",
       "7        3\n",
       "9        3\n",
       "        ..\n",
       "43328    3\n",
       "43329    3\n",
       "43333    3\n",
       "43334    1\n",
       "43336    1\n",
       "Name: ContractCosts_Binned, Length: 24577, dtype: category\n",
       "Categories (4, int64): [1 < 2 < 3 < 4]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Creating bins for Contract Cost\n",
    "\n",
    "\n",
    "Train_data['ContractCost'] = pd.to_numeric(Train_data['ContractCost'])\n",
    "Train_data['ContractCost'] = Train_data['ContractCost'].abs()\n",
    "\n",
    "\n",
    "bin1 = Train_data['ContractCost'].quantile([.25]).values[0]\n",
    "bin2 = Train_data['ContractCost'].quantile([.5]).values[0]\n",
    "bin3 = Train_data['ContractCost'].quantile([.75]).values[0]\n",
    "\n",
    "bins = [-.1, bin1, bin2 , bin3, 1000000000000000000000]\n",
    "labels = [1,2,3,4]\n",
    "Train['ContractCosts_Binned'] = pd.cut(Train['ContractCost'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "Train['ContractCosts_Binned']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract Margin Bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bins contract margin into 4 equally sized based on quartile. Bin 1 is low, Bin 4 is high. This is commented out because in our final model we use contract margin as a continuous variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Contract Margin Bins\n",
    "\n",
    "# Train_data['ContractMargin'] = pd.to_numeric(Train['ContractMargin'])\n",
    "# # Train['ContractMargin'] = Train['ContractMargin'].abs()\n",
    "\n",
    "# bin1 = Train_data['ContractMargin'].quantile([.25]).values[0]\n",
    "# bin2 = Train_data['ContractMargin'].quantile([.5]).values[0]\n",
    "# bin3 = Train_data['ContractMargin'].quantile([.75]).values[0]\n",
    "\n",
    "# bins = [-1,0, bin1, bin2 , bin3, 1000000000000000000000]\n",
    "# labels = [1,2,3,4,5]\n",
    "# Train['ContractMargin_Binned'] = pd.cut(Train['ContractMargin'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "# Train['ContractMargin_Binned']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasted Revenue Bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bins forecasted revenue into 4 equally sized based on quartile. Bin 1 is low, Bin 4 is high. This is commented out because in our final model we omitted forecasted revenue due to overwhelming nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Binning Forecasted Revenue\n",
    "\n",
    "# Train_data['ForecastedRevenue'] = pd.to_numeric(Train['ForecastedRevenue'])\n",
    "# bin1 = Train_data['ForecastedRevenue'].quantile([.04]).values[0]\n",
    "# bin2 = Train_data['ForecastedRevenue'].quantile([.5]).values[0]\n",
    "# bin3 = Train_data['ForecastedRevenue'].quantile([.90]).values[0]\n",
    "\n",
    "# bins = [-.1, bin1, bin2 , bin3, 1000000000000000000000]\n",
    "# labels = [1,2,3,4]\n",
    "# Train['ForecastedRevenue_Binned'] = pd.cut(Train['ForecastedRevenue'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "# Train['ForecastedRevenue_Binned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation for SourceSystem and SOW Revisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunk imputes SourceSystem with the mode value (SFDC - Dell Main) and fills null SOW Revisions with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation\n",
    "\n",
    "Train['SourceSystem'] = Train['SourceSystem'].fillna(Train['SourceSystem'].mode().iloc[0])\n",
    "Train['NumberOfSowRevisions'] = Train.NumberOfSowRevisions.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        SFDC - Dell Main\n",
       "3        SFDC - Dell Main\n",
       "6        SFDC - Dell Main\n",
       "7        SFDC - Dell Main\n",
       "9        SFDC - Dell Main\n",
       "               ...       \n",
       "43328    SFDC - Dell Main\n",
       "43329    SFDC - Dell Main\n",
       "43333     SFDC - EMC Core\n",
       "43334    SFDC - Dell Main\n",
       "43336    SFDC - Dell Main\n",
       "Name: SourceSystem, Length: 24577, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train['SourceSystem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technology Time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we bin technologies into 5 hierarchical bins based on average time to solution for the technology. Bin 1 is low time to solution, Bin 5 is high. The values bin1, bin2, bin3, bin4 will show the boundaries of the bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Train_data.groupby(['TechnologyName']).mean('TimeToSolution')\n",
    "bin1 = df['TimeToSolution'].quantile([.20])\n",
    "bin2 = df['TimeToSolution'].quantile([.40])\n",
    "bin3 = df['TimeToSolution'].quantile([.60])\n",
    "bin4 = df['TimeToSolution'].quantile([.80])\n",
    "\n",
    "bin_one = df[(df['TimeToSolution'] > -0.1) & (df['TimeToSolution'] <= bin1.iloc[0])].index\n",
    "bin_two = df[(df['TimeToSolution'] > bin1.iloc[0]) & (df['TimeToSolution'] <= bin2.iloc[0])].index\n",
    "bin_three = df[(df['TimeToSolution'] > bin2.iloc[0]) & (df['TimeToSolution'] <= bin3.iloc[0])].index\n",
    "bin_four = df[(df['TimeToSolution'] > bin3.iloc[0]) & (df['TimeToSolution'] <= bin4.iloc[0])].index\n",
    "bin_five = df[(df['TimeToSolution'] > bin4.iloc[0]) & (df['TimeToSolution'] <= 10000000000)].index\n",
    "Train['TechnologyComplexity'] = 0\n",
    "\n",
    "for i in range (0,len(Train)):\n",
    "    if Train['TechnologyName'].iloc[i] in bin_one:\n",
    "        Train['TechnologyComplexity'].iloc[i] = 1\n",
    "    elif Train['TechnologyName'].iloc[i] in bin_two:\n",
    "        Train['TechnologyComplexity'].iloc[i] = 2\n",
    "    elif Train['TechnologyName'].iloc[i] in bin_three:\n",
    "        Train['TechnologyComplexity'].iloc[i] = 3\n",
    "    elif Train['TechnologyName'].iloc[i] in bin_four:\n",
    "        Train['TechnologyComplexity'].iloc[i] = 4\n",
    "    elif Train['TechnologyName'].iloc[i] in bin_five:\n",
    "        Train['TechnologyComplexity'].iloc[i] = 5\n",
    "    else:\n",
    "        Train['TechnologyComplexity'].iloc[i] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we bin customers into 5 hierarchical bins based on average time to solution for the customer. Bin 1 is low time to solution, Bin 5 is high. The values bin1, bin2, bin3, bin4 will show the boundaries of the bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Train_data.groupby(['CustomerNameEncrypted']).mean('TimeToSolution')\n",
    "bin1 = df['TimeToSolution'].quantile([.20])\n",
    "bin2 = df['TimeToSolution'].quantile([.40])\n",
    "bin3 = df['TimeToSolution'].quantile([.60])\n",
    "bin4 = df['TimeToSolution'].quantile([.80])\n",
    "\n",
    "c_bin_one = df[(df['TimeToSolution'] > -0.1) & (df['TimeToSolution'] <= bin1.iloc[0])].index\n",
    "c_bin_two = df[(df['TimeToSolution'] > bin1.iloc[0]) & (df['TimeToSolution'] <= bin2.iloc[0])].index\n",
    "c_bin_three = df[(df['TimeToSolution'] > bin2.iloc[0]) & (df['TimeToSolution'] <= bin3.iloc[0])].index\n",
    "c_bin_four = df[(df['TimeToSolution'] > bin3.iloc[0]) & (df['TimeToSolution'] <= bin4.iloc[0])].index\n",
    "c_bin_five = df[(df['TimeToSolution'] > bin4.iloc[0]) & (df['TimeToSolution'] <= 1000000)].index\n",
    "Train['CustomerComplexity'] = 0\n",
    "\n",
    "for i in range (0,len(Train)):\n",
    "    if Train['CustomerNameEncrypted'].iloc[i] in c_bin_one:\n",
    "        Train['CustomerComplexity'].iloc[i] = 1\n",
    "    elif Train['CustomerNameEncrypted'].iloc[i] in c_bin_two:\n",
    "        Train['CustomerComplexity'].iloc[i] = 2\n",
    "    elif Train['CustomerNameEncrypted'].iloc[i] in c_bin_three:\n",
    "        Train['CustomerComplexity'].iloc[i] = 3\n",
    "    elif Train['CustomerNameEncrypted'].iloc[i] in c_bin_four:\n",
    "        Train['CustomerComplexity'].iloc[i] = 4\n",
    "    elif Train['CustomerNameEncrypted'].iloc[i] in c_bin_five:\n",
    "        Train['CustomerComplexity'].iloc[i] = 5\n",
    "    else:\n",
    "        Train['CustomerComplexity'].iloc[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Deal Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we bin customers into 5 hierarchical bins based on the historical deal count for the customer. Bin 1 is low deal count, Bin 5 is high. The values bin1, bin2, bin3, bin4 will show the boundaries of the bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Train_data.groupby(['CustomerNameEncrypted']).count()\n",
    "bin1 = df['TimeToSolution'].quantile([.70])\n",
    "bin2 = df['TimeToSolution'].quantile([.87])\n",
    "bin3 = df['TimeToSolution'].quantile([.933])\n",
    "bin4 = df['TimeToSolution'].quantile([.966])\n",
    "\n",
    "cd_bin_one = df[(df['TimeToSolution'] > -0.1) & (df['TimeToSolution'] <= bin1.iloc[0])].index\n",
    "cd_bin_two = df[(df['TimeToSolution'] > bin1.iloc[0]) & (df['TimeToSolution'] <= bin2.iloc[0])].index\n",
    "cd_bin_three = df[(df['TimeToSolution'] > bin2.iloc[0]) & (df['TimeToSolution'] <= bin3.iloc[0])].index\n",
    "cd_bin_four = df[(df['TimeToSolution'] > bin3.iloc[0]) & (df['TimeToSolution'] <= bin4.iloc[0])].index\n",
    "cd_bin_five = df[(df['TimeToSolution'] > bin4.iloc[0]) & (df['TimeToSolution'] <= 1000000)].index\n",
    "Train['CustomerDealCount'] = 0\n",
    "\n",
    "for i in range (0,len(Train)):\n",
    "    if Train['CustomerNameEncrypted'].iloc[i] in cd_bin_one:\n",
    "        Train['CustomerDealCount'].iloc[i] = 1\n",
    "    elif Train['CustomerNameEncrypted'].iloc[i] in cd_bin_two:\n",
    "        Train['CustomerDealCount'].iloc[i] = 2\n",
    "    elif Train['CustomerNameEncrypted'].iloc[i] in cd_bin_three:\n",
    "        Train['CustomerDealCount'].iloc[i] = 3\n",
    "    elif Train['CustomerNameEncrypted'].iloc[i] in cd_bin_four:\n",
    "        Train['CustomerDealCount'].iloc[i] = 4\n",
    "    elif Train['CustomerNameEncrypted'].iloc[i] in cd_bin_five:\n",
    "        Train['CustomerDealCount'].iloc[i] = 5\n",
    "    else:\n",
    "        Train['CustomerDealCount'].iloc[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country Bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we bin countries into 5 hierarchical bins based on the deal count for each country. Bin 1 is low deal count, Bin 5 is high deal count. The values bin1, bin2, bin3, bin4 will show the boundaries of the bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Train_data.groupby(['CountryValue']).count()\n",
    "bin1 = df['TimeToSolution'].quantile([.20])\n",
    "bin2 = df['TimeToSolution'].quantile([.40])\n",
    "bin3 = df['TimeToSolution'].quantile([.60])\n",
    "bin4 = df['TimeToSolution'].quantile([.80])\n",
    "\n",
    "country_bin_one = df[(df['TimeToSolution'] > -0.1) & (df['TimeToSolution'] <= bin1.iloc[0])].index\n",
    "country_bin_two = df[(df['TimeToSolution'] > bin1.iloc[0]) & (df['TimeToSolution'] <= bin2.iloc[0])].index\n",
    "country_bin_three = df[(df['TimeToSolution'] > bin2.iloc[0]) & (df['TimeToSolution'] <= bin3.iloc[0])].index\n",
    "country_bin_four = df[(df['TimeToSolution'] > bin3.iloc[0]) & (df['TimeToSolution'] <= bin4.iloc[0])].index\n",
    "country_bin_five = df[(df['TimeToSolution'] > bin4.iloc[0]) & (df['TimeToSolution'] <= 1000000)].index\n",
    "Train['CountryCount'] = 0\n",
    "\n",
    "for i in range (0,len(Train)):\n",
    "    if Train['CountryValue'].iloc[i] in country_bin_one:\n",
    "        Train['CountryCount'].iloc[i] = 1\n",
    "    elif Train['CountryValue'].iloc[i] in country_bin_two:\n",
    "        Train['CountryCount'].iloc[i] = 2\n",
    "    elif Train['CountryValue'].iloc[i] in country_bin_three:\n",
    "        Train['CountryCount'].iloc[i] = 3\n",
    "    elif Train['CountryValue'].iloc[i] in country_bin_four:\n",
    "        Train['CountryCount'].iloc[i] = 4\n",
    "    elif Train['CountryValue'].iloc[i] in country_bin_five:\n",
    "        Train['CountryCount'].iloc[i] = 5\n",
    "    else:\n",
    "        Train['CountryCount'].iloc[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpportunityTypeValue Bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we bin opportunity types into 5 hierarchical bins based on average time to solution for the opportunity type. Bin 1 is low time to solution, Bin 5 is high. The values bin1, bin2, bin3, bin4 will show the boundaries of the bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Train_data.groupby(['OpportunityTypeValue']).mean('TimeToSolution')\n",
    "bin1 = df['TimeToSolution'].quantile([.20])\n",
    "bin2 = df['TimeToSolution'].quantile([.40])\n",
    "bin3 = df['TimeToSolution'].quantile([.60])\n",
    "bin4 = df['TimeToSolution'].quantile([.80])\n",
    "\n",
    "o_bin_one = df[(df['TimeToSolution'] > -0.1) & (df['TimeToSolution'] <= bin1.iloc[0])].index\n",
    "o_bin_two = df[(df['TimeToSolution'] > bin1.iloc[0]) & (df['TimeToSolution'] <= bin2.iloc[0])].index\n",
    "o_bin_three = df[(df['TimeToSolution'] > bin2.iloc[0]) & (df['TimeToSolution'] <= bin3.iloc[0])].index\n",
    "o_bin_four = df[(df['TimeToSolution'] > bin3.iloc[0]) & (df['TimeToSolution'] <= bin4.iloc[0])].index\n",
    "o_bin_five = df[(df['TimeToSolution'] > bin4.iloc[0]) & (df['TimeToSolution'] <= 1000000)].index\n",
    "Train['OpportunityTypeComplexity'] = 0\n",
    "\n",
    "for i in range (0,len(Train)):\n",
    "    if Train['OpportunityTypeValue'].iloc[i] in o_bin_one:\n",
    "        Train['OpportunityTypeComplexity'].iloc[i] = 1\n",
    "    elif Train['OpportunityTypeValue'].iloc[i] in o_bin_two:\n",
    "        Train['OpportunityTypeComplexity'].iloc[i] = 2\n",
    "    elif Train['OpportunityTypeValue'].iloc[i] in o_bin_three:\n",
    "        Train['OpportunityTypeComplexity'].iloc[i] = 3\n",
    "    elif Train['OpportunityTypeValue'].iloc[i] in o_bin_four:\n",
    "        Train['OpportunityTypeComplexity'].iloc[i] = 4\n",
    "    elif Train['OpportunityTypeValue'].iloc[i] in o_bin_five:\n",
    "        Train['OpportunityTypeComplexity'].iloc[i] = 5\n",
    "    else:\n",
    "        Train['OpportunityTypeComplexity'].iloc[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset data based on the variables we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyDealData = Train[['Won','RegionValue','Fiscal_Quarter','ContractCosts_Binned','ContractMargin','DeliverableTypeValue',\n",
    "                   'OpportunityTypeComplexity','Governance','IsFederal','SourceSystem','DealType','HasInnerTechnologies','NumberOfSowRevisions',\n",
    "                    'DealTechnologies','CountryCount','TechnologyComplexity','CustomerComplexity','CustomerDealCount']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best to export the data at this point and work from a csv. Otherwise data types may not align in the code below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to replace the path with your own file path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WARNING\n",
    "MyDealData.to_csv(r\"C:\\Users\\justi\\OneDrive\\Documents\\MyDeal_transformed.csv\")\n",
    "### WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Clean data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to replace the path with your own file path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "MyDealData = pd.read_csv(r\"C:\\Users\\justi\\OneDrive\\Documents\\MyDeal_transformed.csv\") \n",
    "MyDealData = MyDealData.drop('Unnamed: 0',1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab info on categoricals so we can transform new data later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categoricals that we will one-hot encode, we need to save the index of levels it takes on in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = MyDealData.RegionValue.value_counts().index.tolist()\n",
    "source = MyDealData.SourceSystem.value_counts().index.tolist()\n",
    "dealtype = MyDealData.DealType.value_counts().index.tolist()\n",
    "deliverabletype = MyDealData.DeliverableTypeValue.value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot encoding for the categorical variables that remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that we aren't taking dummies for the binned variables, they are treated as ordinal for the tree model\n",
    "MyDealData = pd.get_dummies(data=MyDealData, columns = ['RegionValue','SourceSystem','DealType','DeliverableTypeValue'\n",
    "                    ], \\\n",
    "                                   prefix = ['RegionValue','SourceSystem','DealType','DeliverableTypeValue'\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the data into X and y matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data = MyDealData\n",
    "\n",
    "X_train = Train_data.drop('Won',1)\n",
    "y_train = Train_data['Won'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Need to run this feature tuning section once, then fit model with the best params "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two chunks runs cross validation over a grid of different parameters to choose the Random Forest Parameters that optimizer performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:   30.0s remaining:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   ccp_alpha=0.0,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   max_samples=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators=100,\n",
       "                                                   n_jobs=None, oob_score=Fals...\n",
       "                   iid='deprecated', n_iter=5, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [5, 15, 26, 36, 47, 57, 68,\n",
       "                                                      78, 89, 99, 110, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [10, 231, 452, 673, 894,\n",
       "                                                         1115, 1336, 1557, 1778,\n",
       "                                                         2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "from pprint import pprint\n",
    "# Look at parameters used by\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 5, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can look at the parameters chosen through gridsearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 452,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 89,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our random forest model on the training data with the best parameters chosen from above (note that they are hardcoded). We check accuracy on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75     11440\n",
      "           1       0.77      0.85      0.81     13137\n",
      "\n",
      "    accuracy                           0.78     24577\n",
      "   macro avg       0.79      0.78      0.78     24577\n",
      "weighted avg       0.79      0.78      0.78     24577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 452, min_samples_split = 10, min_samples_leaf = 4, max_depth=89,max_features= 'sqrt', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "prediction_train = clf.predict(X_train)\n",
    "print('Classification Report: Train')\n",
    "print(classification_report(y_train, prediction_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to transform new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to take in data from an excel file and transform it into the format that the model takes in: specifying contract cost bin, one-hot encoding categoricals, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    x['ContractCosts_Binned'] = 0\n",
    "    x['TechnologyComplexity'] = 0\n",
    "    x['CustomerComplexity'] = 0\n",
    "    x['CustomerDealCount'] = 0\n",
    "    x['CountryCount'] = 0\n",
    "    x['OpportunityTypeComplexity'] = 0\n",
    "    x['ContractCost'].fillna(mean_cc)\n",
    "    x['ContractMargin'].fillna(mean_cm)\n",
    "    for i in range (0,(len(x))):\n",
    "        if x.ContractCost[i] <= 4181.75:\n",
    "            x.ContractCosts_Binned[i] = 1\n",
    "        elif x.ContractCost[i] <= 12357:\n",
    "            x.ContractCosts_Binned[i] = 2\n",
    "        elif x.ContractCost[i] <= 38167:\n",
    "            x.ContractCosts_Binned[i] = 3\n",
    "        else:\n",
    "            x.ContractCosts_Binned[i] = 4\n",
    "        try:\n",
    "            int(x.NumberOfSowRevisions[i])\n",
    "        except:\n",
    "            x.NumberOfSowRevisions[i] = 0\n",
    "#         if x.ContractMargin[i] <= 0.29:\n",
    "#             x.ContractMargin_Binned[i] = 1\n",
    "#         elif x.ContractMargin[i] <= 0.4:\n",
    "#             x.ContractMargin_Binned[i] = 2\n",
    "#         elif x.ContractMargin[i] <= 0.5:\n",
    "#             x.ContractMargin_Binned[i] = 3\n",
    "#         else:\n",
    "#             x.ContractMargin_Binned[i] = 4\n",
    "        if x['TechnologyName'][i] in bin_one:\n",
    "            x['TechnologyComplexity'][i] = 1\n",
    "        elif x['TechnologyName'][i] in bin_two:\n",
    "            x['TechnologyComplexity'][i] = 2\n",
    "        elif x['TechnologyName'][i] in bin_three:\n",
    "            x['TechnologyComplexity'][i] = 3\n",
    "        elif x['TechnologyName'][i] in bin_four:\n",
    "            x['TechnologyComplexity'][i] = 4\n",
    "        elif x['TechnologyName'][i] in bin_five:\n",
    "            x['TechnologyComplexity'][i] = 5\n",
    "        else:\n",
    "            x['TechnologyComplexity'][i] = 0\n",
    "        if x['CustomerNameEncrypted'][i] in c_bin_one:\n",
    "            x['CustomerComplexity'][i] = 1\n",
    "        elif x['CustomerNameEncrypted'][i] in c_bin_two:\n",
    "            x['CustomerComplexity'][i] = 2\n",
    "        elif x['CustomerNameEncrypted'][i] in c_bin_three:\n",
    "            x['CustomerComplexity'][i] = 3\n",
    "        elif x['CustomerNameEncrypted'][i] in c_bin_four:\n",
    "            x['CustomerComplexity'][i] = 4\n",
    "        elif x['CustomerNameEncrypted'][i] in c_bin_five:\n",
    "            x['CustomerComplexity'][i] = 5\n",
    "        else:\n",
    "            x['CustomerComplexity'][i] = 0\n",
    "        if x['CustomerNameEncrypted'][i] in cd_bin_one:\n",
    "            x['CustomerDealCount'][i] = 1\n",
    "        elif x['CustomerNameEncrypted'][i] in cd_bin_two:\n",
    "            x['CustomerDealCount'][i] = 2\n",
    "        elif x['CustomerNameEncrypted'][i] in cd_bin_three:\n",
    "            x['CustomerDealCount'][i] = 3\n",
    "        elif x['CustomerNameEncrypted'][i] in cd_bin_four:\n",
    "            x['CustomerDealCount'][i] = 4\n",
    "        elif x['CustomerNameEncrypted'][i] in cd_bin_five:\n",
    "            x['CustomerDealCount'][i] = 5\n",
    "        else:\n",
    "            x['CustomerDealCount'][i] = 0\n",
    "        if x['CountryValue'][i] in country_bin_one:\n",
    "            x['CountryCount'][i] = 1\n",
    "        elif x['CountryValue'][i] in country_bin_two:\n",
    "            x['CountryCount'][i] = 2\n",
    "        elif x['CountryValue'][i] in country_bin_three:\n",
    "            x['CountryCount'][i] = 3\n",
    "        elif x['CountryValue'][i] in country_bin_four:\n",
    "            x['CountryCount'][i] = 4\n",
    "        elif x['CountryValue'][i] in country_bin_five:\n",
    "            x['CountryCount'][i] = 5\n",
    "        else:\n",
    "            x['CountryCount'][i] = 0\n",
    "        if x['OpportunityTypeValue'][i] in o_bin_one:\n",
    "            x['OpportunityTypeComplexity'][i] = 1\n",
    "        elif x['OpportunityTypeValue'][i] in o_bin_two:\n",
    "            x['OpportunityTypeComplexity'][i] = 2\n",
    "        elif x['OpportunityTypeValue'][i] in o_bin_three:\n",
    "            x['OpportunityTypeComplexity'][i] = 3\n",
    "        elif x['OpportunityTypeValue'][i] in o_bin_four:\n",
    "            x['OpportunityTypeComplexity'][i] = 4\n",
    "        elif x['OpportunityTypeValue'][i] in o_bin_five:\n",
    "            x['OpportunityTypeComplexity'][i] = 5\n",
    "        else:\n",
    "            x['OpportunityTypeComplexity'][i] = 0\n",
    "        def month(x):\n",
    "            x = x.month\n",
    "            return x\n",
    "        x['Fiscal_Quarter'] = x.IntakeDate.map(month)\n",
    "        x['Fiscal_Quarter'] = x.Fiscal_Quarter.replace([2,3,4,5,6,7,8,9,10,11,12,1],[1,1,1,2,2,2,3,3,3,4,4,4])\n",
    "        x['Governance'] = np.where(x['GovernanceStatusName']== 'No Governance', 0, 1)\n",
    "        \n",
    "        complexity = x.groupby('MyDealId',as_index=True).size()\n",
    "        complexity_dict = complexity.to_dict()\n",
    "        \n",
    "        x['DealTechnologies'] = x['MyDealId'].map(complexity_dict)\n",
    "        \n",
    "        \n",
    "        x['SourceSystem'] = x['SourceSystem'].fillna('SFDC - Dell Main')\n",
    "        x['NumberOfSowRevisions'] = x.NumberOfSowRevisions.fillna(0)\n",
    "    return x[['RegionValue','Fiscal_Quarter','ContractCosts_Binned','ContractMargin','DeliverableTypeValue',\n",
    "                   'OpportunityTypeComplexity','Governance','IsFederal','SourceSystem','DealType','HasInnerTechnologies','NumberOfSowRevisions',\n",
    "                    'DealTechnologies','CountryCount','TechnologyComplexity','CustomerComplexity','CustomerDealCount','IsPrimary']]\n",
    "        \n",
    "def transform2(x):\n",
    "    for i in regions:\n",
    "        x['RegionValue_'+i] = 0\n",
    "    for i in regions:\n",
    "        for j in range(0,len(x)):\n",
    "            if x['RegionValue'][j] ==i:\n",
    "                x['RegionValue_'+i][j] = 1\n",
    "            else:\n",
    "                x['RegionValue_'+i][j] = 0\n",
    "    for i in source:\n",
    "        x['SourceSystem_'+i] = 0\n",
    "    for i in source:\n",
    "        for j in range(0,len(x)):\n",
    "            if x['SourceSystem'][j] ==i:\n",
    "                x['SourceSystem_'+i][j] = 1\n",
    "            else:\n",
    "                x['SourceSystem_'+i][j] = 0\n",
    "    for i in dealtype:\n",
    "        x['DealType_'+i] = 0\n",
    "    for i in dealtype:\n",
    "        for j in range(0,len(x)):\n",
    "            if x['DealType'][j] ==i:\n",
    "                x['DealType_'+i][j] = 1\n",
    "            else:\n",
    "                x['DealType_'+i][j] = 0\n",
    "    for i in deliverabletype:\n",
    "        x['DeliverableTypeValue_'+i] = 0\n",
    "    for i in deliverabletype:\n",
    "        for j in range(0,len(x)):\n",
    "            if x['DeliverableTypeValue'][j] ==i:\n",
    "                x['DeliverableTypeValue_'+i][j] = 1\n",
    "            else:\n",
    "                x['DeliverableTypeValue_'+i][j] = 0\n",
    "    df_new = x.drop(['RegionValue','SourceSystem','DealType','DeliverableTypeValue'],1)\n",
    "    return df_new\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in What-if Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the set of deals you want win probability predictions for. Replace the file path with your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_deals = pd.read_excel(r\"C:\\Users\\justi\\OneDrive\\Documents\\final_demo1.xlsx\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first cell runs the functions above on the input data to create the variables needed for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1 = transform(new_deals)\n",
    "new2 = transform2(new1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell reorders the columns of the new data to match the order used in the original training of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unscaled data sequence\n",
    "X_test = new2\n",
    "prim = new2.IsPrimary\n",
    "\n",
    "#this line reorders columns\n",
    "X_test = new2.drop(['IsPrimary'],1)[X_train.columns]\n",
    "\n",
    "#re-add IsPrimary\n",
    "X_test['IsPrimary'] = prim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Probability deal will be won"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we output the win probability for the hypothetical deal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win Probability\n",
      "0.5420736457073909\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_test)):\n",
    "    if X_test.IsPrimary[i] == 1:\n",
    "        print(clf.predict_proba(X_test.drop(['IsPrimary'],1))[i][1])\n",
    "    else:\n",
    "        print(clf.predict_proba(X_test.drop(['IsPrimary'],1))[i][1],'*warning* part of a parent deal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
